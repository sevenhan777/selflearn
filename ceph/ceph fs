ceph 文件系统特点：
口采用多实例消除性能瓶颈和提升可靠性。
口采用大型日志文件和延迟删除日志机制提升元数据读写性能。
口将Inode 内嵌至 Dentry 中来提升文件索引效率。
口采用目录分片重新定义命名空间层次结构，并且目录分片可以在 MIDS 实例之间动态迁移，从而实现细粒度的流控和负载均衡机制。

文件系统基础：
>>>superblock
1. 文件系统标识信息
文件系统类型（如 ext4、XFS、FAT32）；
文件系统版本；
UUID（唯一标识）；
文件系统标签（label，用户定义的名字）；
2. 空间管理信息
块大小（block size）；
文件系统的总块数、空闲块数；
inode 总数、空闲 inode 数；
每个块组包含的 inode/block 数量；
>>>inode
1. 文件类型与权限
文件类型（普通文件、目录、符号链接、设备文件等）；
权限（rwx，即读/写/执行权限）；
所有者（用户 UID）；
所属组（GID）；
特殊权限位（如 SUID、SGID、sticky bit）；
2. 时间戳
atime（Access Time）：最近访问时间；
mtime（Modify Time）：最近内容修改时间；
ctime（Change Time）：inode 最近更改时间（比如权限变更）；
一些新文件系统还支持 btime（Birth/创建时间）；
3. 文件大小
文件的总字节数；
对于目录，则是目录项所占空间大小；
4. 链接计数
指向该 inode 的硬链接数量；
当计数为 0 且无进程打开时，文件可被删除；
5. 磁盘块地址（data pointers）
指向存储该文件数据的实际物理块的地址；
一般包括：
直接块指针（Direct blocks）；
一级间接指针；
二级间接指针；
三级间接指针（用于大文件）；
>>>dentry  内存结构， 包括文件名到inode的映射，  文件句柄，和进程相关，每个进程打开一个文件就创建一个文件句柄，文件句柄与inode 是多对一
因为系统有多个进程共同打开一个文件
1. 文件名（名称信息）
当前目录项对应的文件名；
注意：文件名保存在 dentry，而不是 inode 中；
一个 inode 可以有多个 dentry（即多个文件名指向同一个文件，如硬链接）；
2. 关联的 inode 指针
指向该目录项所对应的 inode 结构；
若为符号链接（symlink），也可以为空或特殊处理；
3. 父目录指针
指向上级目录的 dentry（形成目录树）；
支持路径解析（比如 .. 向上）；
理解：向下记录文件名到inode的映射，向上记录父dentry.
inode 是 父子dentry的链接纽带 ，父目录本身也是一个文件，有一个inode来记录他，父目录的dentry记录子文件或子目录名到inode的映射，所以要查找一个
文件，就需要inode 和dentry共同合作一级一级找到文件。

创建一个硬链接确实就是创建一个新的目录项（dentry），来记录文件名与现有 inode 之间的映射关系。

日志的三种模式：
write back  ordered  data

分布式文件系统

要实现分布式文件系统，那么就需要，元数据分布在多个节点上，元数据的分布有几种方式
1. 手动分配到多个节点，不方便
2. hash 能自动降元数据分布在多个节点，但是忽略了元数据本身的热度问题，热点数据可能被分配到一个节点
3. 动态子树分区 根据节点负载情况动态分布元数据到多个节点，不适合大量数据迁移的场景，但是元数据数据量小，迁移快所以可以忽略这个缺点，
他的优点是考虑到了数据的热度，根据数据热度情况做动态迁移

mds:
上面提到，ceph 元数据并不适合hash, 但是其元数据仍存在rados object(rados object 是基于hash的分布)，原因在于，mds将元数据大部分缓存在内存中
所以落到磁盘上是怎么分布的无关紧要，只要内存中是按照动态子树分布就可以了

mds如何管理元数据

传统文件系统一个inode对应一个或多个dentry(硬链接)，ceph 将inode 和 dentry合并成一个数据结构，这样性能会变好（因为找到了dentry也就拿到
了inode，一次读就搞定,不用去其他磁盘或者磁盘的其他位置再加载inode到内存），但是也带来了一个问题，硬链接怎么解决。 

要解决硬链接问题，引入 remote dentry, 硬链接结构存在remote dentry, inode 继续与primary dentry 合并， 硬链接remote dentry 并不包含
inode 而是引入anchor表来记录如何从remote dentry 找到inode.
没有 Anchor 表时，系统必须逐层读取目录结构的 Dentry 和 inode 来解析路径，这会增加网络/磁盘访问成本、影响性能。
而 Anchor 表作为“路径缓存索引”，大幅优化了路径跳转效率

一个inode 对应一个或多个元数据对象， 元数据对象以inode编号命名，
元数据并不直接写到后端rados 对象而是先写到日志再根据策略写到后端
mds 如何处理硬链接 ？？？？

日志：
增加日志的好处
1. 保证数据一致性，完整性
2. 性能方面，减少不必要的提交，如修改后立即撤回了修改，则这次只会在元数据层做出反应，而不会真的提交到rados 对象
3. 合并提交，多个io合并成一个

负载均衡：
1.目录分片
将一个目录进行分片后，如果某一个fragment 下人有子目录，也可继续对其子目录继续分片，不同的分片可以存储在不同的mds上
客户端如何处理？
客户端请求访问 /logs/log123.txt；
它会对文件名 log123.txt 进行 Hash；
确定应该落在哪个 Fragment，比如是 01/2；
然后直接向托管这个 Fragment 的 MDS（例如 MDS 2）发起请求。

示例：使用 FragTree 查找文件所属的 Fragment
🎯 假设
你有一个目录 /bigdir，它太大了，Ceph 把它切成了 4 个片段（Fragments），组成一个 FragTree，如下：

Fragment 编号	掩码（bitmask）表示法	意义（按 Hash 值前缀匹配）
F1	0/1	所有 Hash 以 0 开头的
F2	10/2	所有 Hash 以 10 开头的
F3	110/3	所有 Hash 以 110 开头的
F4	111/3	所有 Hash 以 111 开头的

这些片段构成了一个逻辑上的 前缀匹配树（FragTree）。

📄 文件名 Hash 演示
现在我们来看几个文件名，看看它们如何被映射到某个 Fragment：

文件名	Hash（取前几位）	匹配到的 Fragment
apple.txt	0110...	F1（匹配 0/1）
temp.log	1011...	F2（匹配 10/2）
zebra.jpg	1101...	F3（匹配 110/3）
zulu.mp4	1110...	F4（匹配 111/3）

查找流程如下：
Ceph 对文件名进行 Hash，例如 zebra.jpg → 1101...
在 FragTree 中查找最“精确匹配”的前缀掩码（Longest Prefix Match）：
1101... 匹配到掩码 110/3（前 3 位是 110）
因此，zebra.jpg 的元数据属于 Fragment F3
目录分片是对一个目录，在分割，让一个目录下的热点数据可以分布在多个节点实现负载均衡

》》》根据fragment 和 inode 热度判断某个mds的负载情况，然后进行子树迁移
我的理解是， 一个mds 负载高时可以选择迁移一部分子树， 一个子树热度高时，可以进行子树分片然后迁移部分分片
一、热度统计的目标
CephFS 的设计目标是将元数据 动态均衡分布在多个 MDS 节点之间，避免某个 MDS 因为负责了热点目录而过载。
因此它会持续监控每个目录及其片段的访问“热度”，作为迁移决策的依据。
📊 二、Inode 和 Fragment 的热度指标
🔹 每个 Inode 统计两个方面的热度：
读访问频率（例如 open、stat）
写访问频率（例如 create、rename）
🔹 每个 Fragment 除了上面 Inode 的热度，还额外统计：
readdir 次数：说明这个目录在被浏览
获取元数据的频率：如 stat、lookup
写入对象存储的频率：表示与底层 RADOS 的交互活跃程度
这样，每个 Fragment 拥有更完整的访问画像，用于判断它是不是热点。
🧩 三、子树（Subtree）的多维热度统计（解决深层嵌套访问问题）
当客户端频繁访问深层嵌套目录（比如 /a/b/c/d/e/f.txt），热度可能集中在某个深层目录上。为了合理迁移整个热点结构，
子树内维护三个关键变量来汇总热度：
变量编号	内容	用途
变量1	当前子树内部所有嵌套元数据的总热度	看整个子树整体是否热
变量2	当前 MDS 所“权威持有”的元数据部分的嵌套热度	判断自己真正负责的部分是否热
变量3	当前 MDS 权威的所有子树的热度	用于合并多个子树信息做迁移判断
这样可以判断：
整体是否该迁移；
哪个子部分是热点；
是否需要合并迁移或只迁移局部。
🔁 四、热度传播机制
🔸 热度是“逐级向上传播”的：
客户端每次访问文件：
目标 Inode 热度+1；
它的 祖先目录 Inode 的热度也会递增；
最终热度影响整个路径链上的节点（如从 /a/b/c.txt 向上传播到 /）
这种机制确保了：
访问 /a/b/c.txt 会让 /a、/a/b 也反映热度上升；
为迁移时考虑父子目录关系提供依据。
📬 五、MDS 节点之间的协同决策
每个 MDS 节点会定期：
上报自己的负载（比如内存使用、请求速率等）；
广播每个子树的热度和访问分布信息；
MDS 可以根据这些共享信息：
识别热点子树；
判断迁移是否必要；
决定将哪个子树“迁出”给其他负载较低的 MDS。

权威元数据就是某个 MDS 对某一目录（Fragment）具有独占管理权的元数据拷贝，它能读写、处理客户端请求，
而其他 MDS 即使缓存了相关数据也只能“看”，不能“动”。
