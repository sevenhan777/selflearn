ceph 文件系统特点：
口采用多实例消除性能瓶颈和提升可靠性。
口采用大型日志文件和延迟删除日志机制提升元数据读写性能。
口将Inode 内嵌至 Dentry 中来提升文件索引效率。
口采用目录分片重新定义命名空间层次结构，并且目录分片可以在 MIDS 实例之间动态迁移，从而实现细粒度的流控和负载均衡机制。

文件系统基础：
>>>superblock
1. 文件系统标识信息
文件系统类型（如 ext4、XFS、FAT32）；
文件系统版本；
UUID（唯一标识）；
文件系统标签（label，用户定义的名字）；
2. 空间管理信息
块大小（block size）；
文件系统的总块数、空闲块数；
inode 总数、空闲 inode 数；
每个块组包含的 inode/block 数量；
>>>inode
1. 文件类型与权限
文件类型（普通文件、目录、符号链接、设备文件等）；
权限（rwx，即读/写/执行权限）；
所有者（用户 UID）；
所属组（GID）；
特殊权限位（如 SUID、SGID、sticky bit）；
2. 时间戳
atime（Access Time）：最近访问时间；
mtime（Modify Time）：最近内容修改时间；
ctime（Change Time）：inode 最近更改时间（比如权限变更）；
一些新文件系统还支持 btime（Birth/创建时间）；
3. 文件大小
文件的总字节数；
对于目录，则是目录项所占空间大小；
4. 链接计数
指向该 inode 的硬链接数量；
当计数为 0 且无进程打开时，文件可被删除；
5. 磁盘块地址（data pointers）
指向存储该文件数据的实际物理块的地址；
一般包括：
直接块指针（Direct blocks）；
一级间接指针；
二级间接指针；
三级间接指针（用于大文件）；
>>>dentry  内存结构， 包括文件名到inode的映射，  文件句柄，和进程相关，每个进程打开一个文件就创建一个文件句柄，文件句柄与inode 是多对一
因为系统有多个进程共同打开一个文件
1. 文件名（名称信息）
当前目录项对应的文件名；
注意：文件名保存在 dentry，而不是 inode 中；
一个 inode 可以有多个 dentry（即多个文件名指向同一个文件，如硬链接）；
2. 关联的 inode 指针
指向该目录项所对应的 inode 结构；
若为符号链接（symlink），也可以为空或特殊处理；
3. 父目录指针
指向上级目录的 dentry（形成目录树）；
支持路径解析（比如 .. 向上）；
理解：向下记录文件名到inode的映射，向上记录父dentry.
inode 是 父子dentry的链接纽带 ，父目录本身也是一个文件，有一个inode来记录他，父目录的dentry记录子文件或子目录名到inode的映射，所以要查找一个
文件，就需要inode 和dentry共同合作一级一级找到文件。

创建一个硬链接确实就是创建一个新的目录项（dentry），来记录文件名与现有 inode 之间的映射关系。

分布式文件系统

要实现分布式文件系统，那么就需要，元数据分布在多个节点上，元数据的分布有几种方式
1. 手动分配到多个节点，不方便
2. hash 能自动降元数据分布在多个节点，但是忽略了元数据本身的热度问题，热点数据可能被分配到一个节点
3. 动态子树分区 根据节点负载情况动态分布元数据到多个节点，不适合大量数据迁移的场景，但是元数据数据量小，迁移快所以可以忽略这个缺点，
他的优点是考虑到了数据的热度，根据数据热度情况做动态迁移

mds:
上面提到，ceph 元数据并不适合hash, 但是其元数据仍存在rados object(rados object 是基于hash的分布)，原因在于，mds将元数据大部分缓存在内存中
所以落到磁盘上是怎么分布的无关紧要，只要内存中是按照动态子树分布就可以了

mds如何管理元数据
一个inode 对应一个或多个元数据对象， 元数据对象以inode编号命名，
元数据并不直接写到后端rados 对象而是先写到日志再根据策略写到后端
mds 如何处理硬链接 ？？？？

日志：？？？

负载均衡：
1.目录分片
示例：使用 FragTree 查找文件所属的 Fragment
🎯 假设
你有一个目录 /bigdir，它太大了，Ceph 把它切成了 4 个片段（Fragments），组成一个 FragTree，如下：

Fragment 编号	掩码（bitmask）表示法	意义（按 Hash 值前缀匹配）
F1	0/1	所有 Hash 以 0 开头的
F2	10/2	所有 Hash 以 10 开头的
F3	110/3	所有 Hash 以 110 开头的
F4	111/3	所有 Hash 以 111 开头的

这些片段构成了一个逻辑上的 前缀匹配树（FragTree）。

📄 文件名 Hash 演示
现在我们来看几个文件名，看看它们如何被映射到某个 Fragment：

文件名	Hash（取前几位）	匹配到的 Fragment
apple.txt	0110...	F1（匹配 0/1）
temp.log	1011...	F2（匹配 10/2）
zebra.jpg	1101...	F3（匹配 110/3）
zulu.mp4	1110...	F4（匹配 111/3）

查找流程如下：

Ceph 对文件名进行 Hash，例如 zebra.jpg → 1101...

在 FragTree 中查找最“精确匹配”的前缀掩码（Longest Prefix Match）：

1101... 匹配到掩码 110/3（前 3 位是 110）

因此，zebra.jpg 的元数据属于 Fragment F3



