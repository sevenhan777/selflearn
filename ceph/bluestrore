https://zhuanlan.zhihu.com/p/91015613

一个问题：
write ahead log 是不是真的需要写入到日志盘？
存储的内存一般有电池即使断电后也可以运行一段时间后，可以之建立映射关系而不必真正写入，这样写性能才会更高。

关键点：
 目标背景
BlueStore 是为全闪存设计的 Ceph 存储后端，其核心目标是：

保证 数据一致性（ACID）

减少 写放大与延迟，特别是在全闪存环境中

避免传统双写日志带来的性能问题

🔍 问题本质
全闪存系统中：

写入延迟主要来自传输，不是寻址；

如果 Journal 带来两次传输（写日志 + 写数据），性能反而下降；

但为了确保一致性，仍然不能完全舍弃 Journal。

✅ BlueStore 的写策略选择
场景	策略	原因
非覆盖写	直接分配新空间写入	不影响原数据，无需 Journal
覆盖写（Block 对齐）	COW（Copy-on-Write）	避免修改原位置，天然具备一致性
覆盖写（Block 不对齐）	RMW + Journal	涉及原数据，需要读取并合并，非原子操作，借助日志保障一致性

🧩 各写策略详解
🟢 非覆盖写（append）：
直接写入新位置，不需要删除或重用老数据。

简单高效，不涉及一致性风险。

🟡 COW（对齐覆盖写）：
分配新空间，写入新数据；

修改指针指向新地址；

旧数据空间异步释放。

✅ 优点：

不破坏原数据，可中断恢复；

不需要日志，一致性天然保证。

⚠️ 缺点：

空间碎片化严重；

频繁更新可能使读变得 非顺序、性能下降；（ssd 不受影响）

增加 元数据压力。

🔴 RMW（非对齐覆盖写）：
读取原块；

合并新数据；

写回原位置；

用 Journal 记录这个过程，保证写原位置前先备份。

✅ 优点：

节省空间，不需要额外新块；

减少碎片。

⚠️ 缺点：

额外读放大；

如果无日志，一旦掉电将损坏原数据，故必须配合 Journal。

📦 增量日志优化（Partial Journal）
BlueStore 不再为所有写都使用 Journal，而是只针对：

非对齐小覆盖写的部分（首尾两个不对齐 block）使用 Journal；

中间对齐的部分用 COW；

✅ 优点：

既保证一致性；

又避免 Journal 写放大；

达成性能和可靠性的平衡。

stable_mod
https://zhuanlan.zhihu.com/p/541318768
stable_mod 的工作机制
原始目标：哈希取模

pg_num = hash(object) % PG_TOTAL
优化思路：使用掩码（2^n - 1）代替取模

pg_num_candidate = hash(object) & (2^n - 1)
判断候选结果是否有效：

如果 pg_num_candidate < PG_TOTAL → 合法 PG，直接使用

如果 pg_num_candidate >= PG_TOTAL → 非法（空穴），降一级掩码处理：

pg_num = hash(object) & (2^(n-1) - 1)
💡 为什么会出现“空穴”
当 PG_TOTAL 不是 2 的幂时，比如 12（1100），掩码会用最近的上界 15（1111），那么哈希值与掩码“与运算”后可能落在 12~15 范围，这些 PG 并不存在，称为 空穴。

📉 stable_mod 的副作用：PG 负载不均衡
所有 >= PG_TOTAL 的结果最终都被压缩到 0 ~ 2^(n-1)-1 范围内；

这部分 PG 会 多承担空穴映射过来的对象；

结果：低编号 PG（0~7）负载更高；

长期运行会导致 数据倾斜、PG 热点、重平衡压力上升。

🛠 最佳实践建议
为了最大程度避免上述问题：

✅ 优先确保 PG 总数为 2 的幂（如 256、512、1024）

避免空穴；

保证数据均匀分布；

减少 hash 降级调整带来的冲击。

📌 总结一句话
stable_mod 是 Ceph 为了减少 PG 变动后数据迁移而引入的折中策略，在非 2 的幂次 PG 数量下可以保持映射稳定性，但以数据均衡性为代价，因此生产中应尽量设置 PG 总数为 2 的幂次。

pg 分裂：
https://ivanzz1001.github.io/records/post/ceph/2019/01/03/pg-pgp
============================================================================================================================================
Q: 新增osd 导致数据迁移 和 pg 分裂存在什么关系？
ChatGPT said:
这是个非常关键的问题！新增 OSD 导致的数据迁移和PG 分裂（PG split）确实都可能导致数据重新分布，但它们在机制和目的上并不相同，关系如下：

🧠 先明确两者的定义：
✅ 新增 OSD 导致的数据迁移：
触发时机：当新增 OSD 后，CRUSH 映射变化，原来属于旧 OSD 的一部分 PG 现在要“搬家”到新 OSD；

迁移粒度：PG 为单位；

本质：同样数量的 PG 被重新分布到更多 OSD 上，使得负载更均衡；

PG 的总数没有变！

✅ PG 分裂（PG split）：
触发时机：通常在 pool 的数据量增大后，人为或自动调整 pool 的 PG 数量（如从 128 提升到 256）；

迁移粒度：PG 被一分为二，原来的一个 PG 会变成两个 PG（例如 PG 1.2 → PG 1.2 和 1.a2）；

本质：PG 的总数变多，从而更细粒度地分摊数据，提升并发能力；

分裂后，部分对象也会从原来的 PG 迁移到新 PG 对应的 OSD 上。

📌 这两者的关键区别与联系如下：
项目	新增 OSD	PG 分裂
是否引起数据迁移	✅ 是（CRUSH 重映射）	✅ 是（PG 被拆分）
是否改变 PG 数量	❌ 否	✅ 是
是否触发 CRUSH 重映射	✅ 是	✅ 是（PG ID 变化后也需重映射）
是否会相互影响	✅ 可能间接影响（见下）	✅ 会引起更多数据重映射
数据重分布程度	通常较轻微（部分 PG）	较大（所有 PG 可能拆分）

🔄 二者的相互关系：
新增 OSD 不会自动引发 PG 分裂，但可能促使你考虑 PG 分裂：

例如，原本 128 个 PG 平均分布在 8 个 OSD 上；

新增到 16 个 OSD 后，每个 OSD 分不到足够的 PG，负载不均衡；

这时候就要 手动提高 PG 数（PG 分裂），让新 OSD 分到更多 PG。

PG 分裂后再新增 OSD，迁移会更平滑：

因为 PG 越多，单位 PG 数据量越小，迁移过程对系统压力越小。

✅ 总结一句话：
新增 OSD 导致的是 PG 的重映射，不改变 PG 数量；PG 分裂是显式地增加 PG 数量，两者都会引发数据迁移，但出发点、机制、影响不同。

=========================================================================================================================================
object:

一个 object 有多个 extent 组成， 多个extent 逻辑连续，但是物理磁盘上不连续
extent{
offset,     //逻辑的offset 从0开始，多个extent 起始值不同，一起组成连续的object 空间
lenth,
data
}
data{
offset,   //物理磁盘上的偏移地址
lenth
}

数据校验：
以extent 为单位，计算校验和，校验和使用kvdb(原子性保证校验数据不出错)存储，读取数据时重新计算校验和与存储的校验和做对比
校验算法：
在hash冲突和计算效率中做权衡 crc

数据压缩：
压缩要在压缩率和压缩效率间做权衡
压缩对性能的影响：
 1. 覆盖写，需要先读出压缩数据，然后解压缩，再写入，写性能较差
 2. 重新分配一块extent给新写入的数据，但是新分配的extent
方案二：重定向写（Copy-on-Write）
流程：

不对旧的压缩块进行修改；

直接新分配一个 extent（哪怕只修改了一小部分）；

将旧数据 + 新数据组合，压缩后写到新位置；

更新元数据指向新位置。

优点：不用解压旧数据，写操作快。

缺点：

空间浪费：原来的 extent 没完全无效，却要全部替换；

元数据膨胀：频繁小写会不断生成新 extent，导致碎片多、管理成本高；

读性能下降：读取数据可能要追踪多个 extent 合并（多个版本），不像一整块数据那么连续；

可能重复压缩相同数据：同一部分数据被复制进多个新 extent，违背了压缩节省空间的目的
===========================================================================================================================================
在 Ceph 的 BlueStore 中，Blob 是底层数据存储结构中的一个核心概念，用于管理 一组连续的、共享同一压缩方式和分配特性的数据块。
Blob 的设计可以高效地组织和管理数据，特别是在面对写放大、压缩、COW（Copy-on-Write）等复杂问题时。

🧩 一、Blob 的定义
在 BlueStore 中：

一个 Blob 表示一块物理设备上分配的空间；

一个 Blob 可能对应多个逻辑数据的 extent（块）；

所有映射到同一个 Blob 的 extent 共享：

相同的压缩设置（是否压缩、压缩算法）；

相同的存储分配策略（比如是否写时复制）；

相同的缓存策略等。

🧱 二、Blob 的核心字段（简化说明）
class Blob {
    uint64_t id;                // Blob ID
    uint64_t offset;            // 起始偏移（设备上的位置）
    uint64_t length;            // Blob 长度
    bool compressed;            // 是否压缩
    string compression_type;    // 压缩算法（zlib、snappy、zstd）
    vector<extent_map> extents; // 映射的 extent 表（逻辑到物理）
    bitmap used;                // 标识哪些区域在用
};
🧰 三、Blob 与 BlueStore 的结构关系
一个 BlueStore 对象（Object） → 会映射为若干个 extent（逻辑块）
这些 extent → 分配到若干个 Blob 上（通常由 Onode 管理）

结构如下：
BlueStore Object
 ├── Onode（元数据节点）
 │     └── extent_map（逻辑偏移 → Blob 映射）
 └── Blob(s)
       └── 物理存储空间（Raw Device）上的块
🌀 四、Blob 的作用
✅ 数据压缩支持
Blob 是压缩操作的基本单位。多个 extent 如果落在同一个 blob 中，必须使用相同的压缩设置。

✅ 写时复制（COW）支持
当某个 extent 需要被修改（覆盖写）时，BlueStore 可能会新分配 blob 空间，而不是修改原 blob，实现 copy-on-write。

✅ 降低碎片
通过 blob 管理多个 extent，避免每个写入都独立申请设备空间，从而减少碎片、提高空间利用率。

✅ 提高缓存效率
因为 blob 是物理空间的连续块，便于整体缓存和预读优化。

🎯 举个例子
假设你有一个 1MB 的对象，它被分成如下三个 extent：

extent 1（0–256KB）

extent 2（256–512KB）

extent 3（512–1MB）

BlueStore 可能将它们分配到两个 blob 中：

blob_1 管理 extent 1 和 extent 2（512KB，压缩类型 zstd）

blob_2 管理 extent 3（512KB，不压缩）

这样可以根据数据特点对 blob 分别进行压缩或跳过压缩，提高整体性能。


映射过程概述
对象层级（Object）

每个对象包含一个或多个 Extent（逻辑地址段）。

Extent 指定对象数据的逻辑偏移和长度。

Extent 映射到 Blob

每个 Extent 指向一个 Blob 及其在 Blob 中的偏移。

同一个 Blob 可以被多个 Extent 共享（典型于压缩场景）。

Blob 映射到物理空间

Blob 管理自身的物理布局，即：

存储在哪些 块设备偏移 上；

是否使用压缩、加密等。

逻辑地址 ➝ 物理地址的完整链路：

[Object Offset] ➝ Extent ➝ Blob ➝ Physical Offset on Device
🛠️ Blob 的内部映射细节
bluestore_blob_t 结构体（简化）：

struct bluestore_blob_t {
    uint64_t physical_offset;  // 在块设备上的偏移
    uint64_t length;           // Blob 总长度
    uint32_t flags;            // 是否压缩、加密等
    map<uint32_t, uint64_t> logical_to_physical; // offset mapping
};
关键字段说明：

physical_offset: 该 Blob 在设备上的起始偏移。

logical_to_physical: 如果使用压缩，逻辑偏移 ≠ 物理偏移；否则为线性映射。

📦 示例说明
假设你写了一个对象的 0–128KB 数据段：

BlueStore 会把它分为一个 Extent，起始逻辑偏移为 0。

这个 Extent 可能指向 Blob 42，偏移为 0。

Blob 42 的 physical_offset = 1MB。

如果未压缩：最终物理地址 = 1MB + 0 = 1MB； 如果压缩了：需查 logical_to_physical 映射找到偏移。

应用写请求 (obj1, offset 0, len 8KB)
     ↓
BlueStore buffer
     ↓
分配 Blob A
     ↓
Blob A 分配 Block1, Block2 （物理地址）
     ↓
数据写入 block device
     ↓
Blob A 记录 Blob Offset → Block Offset 映射
     ↓
Object 的 Extent Map 记录 Logical Offset → Blob A 映射
     ↓
Extent Map 持久化到 RocksDB

客户端读取 obj1 (offset 0, len 8KB)
     ↓
BlueStore 查询 RocksDB
     ↓
拿到 obj1 的 extent map
     ↓
找到 Blob A
     ↓
Blob A 映射出 block device 上的偏移
     ↓
读取 block 设备的数据
     ↓
解压缩（如果需要）
     ↓
数据组装返回客户端
==========================================================================================================
object extent map  extent  blob 关系图：
Object
 └── Extent Map
      ├── Extent1: [offset=0, length=4K] → BlobA @ offset=0
      ├── Extent2: [offset=4K, length=4K] → BlobA @ offset=4K
      ├── Extent3: [offset=8K, length=8K] → BlobB @ offset=0
      └── Extent4: [offset=20K, length=4K] → BlobC @ offset=0

BlobA: 8K (shared by extent1, extent2)
BlobB: 8K (used by extent3)
BlobC: 4K (used by extent4)

================================================================================================
在 Ceph 的 BlueStore 存储引擎中，onode（object node）是每个对象（object）对应的一份元数据结构，
用来描述该对象的所有核心信息，包括数据布局、状态、快照等。它是 BlueStore 中最关键的元数据之一。

✅ 一句话理解：
onode 是对象的“头部元数据”，记录该对象的状态和数据的组织结构（如 extent map 的位置、版本、快照信息等）。

总体结构关系图（逻辑层次）

Object（用户级对象）
│
├── onode（元数据头）
│   ├── size、version、flags ...
│   ├── extent map（可以在 onode 内部或外部分片）
│   │   ├── extent（逻辑段）
│   │   │   ├── 映射到一个或多个 blob
│   │   │   └── 记录逻辑地址 ↔ 物理 blob 之间的关系
│   └── spanning_blob_map（用于特殊跨片 blob）
│
└── blob（底层数据块）
    ├── 存放原始或压缩数据
    └── 可以被多个 extent 引用（节省空间）
🔍 2. 组件关系解释
✅ object（对象）
BlueStore 中一个对象就是用户数据的容器，如某个 RBD 块或 RGW 对象。

✅ onode（对象元数据）
每个对象对应一个 onode，存放：

对象大小、版本号、快照信息

对象的 extent map 根节点（或分片索引）

数据完整性校验码

blob 引用统计

spanning blob map（跨 extent map 分片的 blob）

✅ extent map（逻辑段索引表）
类似一个“段表”，记录对象的逻辑地址范围映射到哪些物理 blob。

一个 extent 表示对象中一段逻辑数据：

例如 offset=4096, length=8192，表示 [4K, 12K) 这一段

extent 内部不会直接保存数据，而是引用 blob 中的某个物理区域。

✅ extent（逻辑段）
是 extent map 中的一个元素，描述对象的某个逻辑区域。

每个 extent 映射到一个或多个 blob 及其在 blob 中的偏移（因为物理空间可能不连续）。

✅ blob（物理数据块）
实际存储用户数据的单元，可能是原始数据，也可能是压缩过的。

特点：

可以被多个 extent 引用（节省空间）

存储在物理设备（如 SSD）上

每个 blob 中还维护自己的 pextent（物理 extent）映射

📦 举个例子理解
假设我们向一个对象写入如下内容：

offset = 0, length = 4K：写入数据 A

offset = 8K, length = 4K：写入数据 B

构造流程：
onode 被初始化，记录对象版本和逻辑大小为 12K；

extent map 中插入两个 extent：

extent1：offset=0, length=4K → 指向 blob1 中的一段

extent2：offset=8K, length=4K → 指向 blob2 中的一段

blob1 和 blob2 是独立压缩块，存储数据 A 和 B；

如果某天数据 A 和 B 被合并压缩到了一个新 blob3，extent1 和 extent2 都可以重新指向 blob3 中不同偏移位置。

=========================================================================================================================================
blob 的数据校验码（chunk CRC）是用于校验实际数据块的完整性；

onode 的数据校验码是用于校验对象元数据（如 extent map）的完整性；

它们是两个独立的层级，校验的对象不同，但共同保障 object 的全局完整性。

🔍 详细区分：
校验码位置	作用范围	校验对象	使用时机
blob 校验码	校验 object 的用户数据	blob 中的数据块（chunk，一般4K）	读取 blob 数据时
onode 校验码	校验 object 的元数据完整性	onode 结构体本身（如属性、extent map）	读取 onode 信息时

🧱 举个例子来说明：
设你有一个 object：

写入 8KB 数据；

BlueStore 会创建：

一个 onode（描述这个 object 的结构，包含 extent map、属性等）；

一个或多个 blob（实际存储数据）；

onode 的元数据部分可能会有一个完整的 checksum（例如 crc32c） 来保障：

extent map 没有被篡改；

object 的属性（如 mtime、size）没有损坏；

blob 存储的那 8KB 会被分成两个 4K 块：

每个 4K 块会有自己的 CRC 校验值，单独存在 blob 的 metadata 区域；

读取 blob 时，每个 4K 块都单独校验。

🤔 为什么要有两个校验码？
因为数据完整性保护是分层的：

blob 层负责保障「数据是否损坏」；

onode 层负责保障「索引信息是否准确」；

如果只做数据校验（blob），但索引出错（onode）仍然找不到正确的数据。

====================================================================================================================================
cache:
 2Q 算法基本结构
2Q 缓存系统维护 三个队列：

队列名称	功能	大致内容
A1in	最近进入的缓存页	新加入但尚未被访问第二次的页
A1out	被移出 A1in 的“痕迹”	表示这个页曾被访问过一次（只存页号，不存数据）
Am	热页缓存队列	被访问两次及以上的“活跃”页

🔁 缓存操作流程
🔸 1. 缓存访问（命中或不命中）：
访问命中 Am：更新其位置，类似 LRU（最近访问放在队尾）。

访问命中 A1in：

表示访问了第二次 → 将其移动到 Am。

访问命中 A1out：

说明它被访问第二次（再次活跃），将其从磁盘加载到 Am。

首次访问（不在三队中）：

插入到 A1in；

如果 A1in 满，移出 LRU 页（头部页），并记录其页号到 A1out；

如果 A1out 满，也以 FIFO 策略丢弃最旧记录。

🔸 2. 页面淘汰策略
当缓存空间不足时：

先从 A1in 淘汰；

如果 A1in 太小，再从 Am 中淘汰；

淘汰时遵循 LRU 原则（从头部淘汰）。

📊 队列大小推荐
A1in + A1out：共占用约 1/3 的缓存空间；

Am：占用约 2/3 的缓存空间；

A1out 只记录页号，不占数据页容量。

ARC 的核心理念：
ARC 同时维护两个 LRU 链表：

T1：用于记录最近访问过但只被访问过一次的缓存项（最近性）；

T2：用于记录被访问过两次或更多次的缓存项（频率）；

同时还有两个「幽灵链表」记录被移出缓存但仍值得追踪的 key：

B1：记录被从 T1 移除的 key（表示最近访问过但频率不高）；

B2：记录被从 T2 移除的 key（表示频繁访问过）。

🧠 工作机制：
ARC 会动态地在 T1 和 T2 之间分配空间，根据访问模式来调整优先策略：

如果近期访问的对象经常来自 B1（即缓存中新近的页面反复被访问），就说明**「最近性」更重要**，ARC 会扩大 T1；

如果命中多来自 B2（即频繁访问的页面反复访问），就说明**「频率」更重要**，ARC 会扩大 T2；

因此，ARC 能够自适应地权衡「最近使用」和「频繁使用」，不像传统 LRU 或 LFU 只能一条路走到底
